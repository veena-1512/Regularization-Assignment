{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3caa9a0-a870-4112-8ecb-fa4702e41375",
   "metadata": {},
   "source": [
    "                Part l: Upder_tapdipg Regularizatioo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d24c0ee-7175-4ee2-b763-b62ff98573ae",
   "metadata": {},
   "source": [
    "Q1. What is regularization in the context of deep learning? Why is it important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a82a674-fabf-456f-b4aa-864a6dc86760",
   "metadata": {},
   "source": [
    "Regularization in the context of deep learning refers to a set of techniques used to prevent overfitting, which occurs when a model learns to memorize the training data rather than generalize well to new, unseen data. Overfitting typically happens when a model becomes too complex relative to the amount and noise level of the training data.\n",
    "\n",
    "Regularization techniques introduce additional constraints or penalties on the model parameters during training to discourage complex or erratic behavior. The most common types of regularization techniques used in deep learning include:\n",
    "\n",
    "1. L1 Regularization (Lasso): This technique adds a penalty term to the loss function proportional to the absolute values of the model weights. It encourages sparsity in the weight matrix, effectively driving some of the weights to zero.\n",
    "\n",
    "2. L2 Regularization (Ridge): L2 regularization adds a penalty term to the loss function proportional to the squared magnitudes of the model weights. It discourages large weights and promotes smoother models by penalizing large deviations from zero.\n",
    "\n",
    "3. Dropout: Dropout is a technique where randomly selected neurons are ignored during training. This helps prevent co-adaptation of neurons and forces the network to learn more robust features.\n",
    "\n",
    "4. Early Stopping: Early stopping involves monitoring the model's performance on a separate validation dataset during training and stopping the training process once the performance starts to degrade. This prevents the model from overfitting to the training data by stopping training before it learns to memorize it.\n",
    "\n",
    "5. Data Augmentation: Data augmentation involves artificially increasing the size of the training dataset by applying transformations such as rotation, translation, scaling, etc., to the input data. This helps expose the model to a wider variety of input patterns and reduces overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4816c84-85e4-415a-8264-73bfdf7c0aed",
   "metadata": {},
   "source": [
    "Q2. Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e6be4-9f60-43c5-bc1d-30b3920e90af",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes the balance between the bias of the model and its variance. Here's an explanation of each component:\n",
    "\n",
    "1. Bias: Bias refers to the error introduced by approximating a real-world problem with a simplified model. A model with high bias tends to oversimplify the problem and may underfit the data. High bias models typically have low complexity and may not capture all the underlying patterns in the data.\n",
    "\n",
    "2. Variance: Variance refers to the model's sensitivity to small fluctuations or noise in the training data. A model with high variance is overly sensitive to the training data and may capture noise as if it were signal, leading to overfitting. High variance models typically have high complexity and may perform well on the training data but generalize poorly to new, unseen data.\n",
    "\n",
    "The bias-variance tradeoff arises because decreasing one component (e.g., bias) often leads to an increase in the other component (e.g., variance). Finding the right balance between bias and variance is crucial for building models that generalize well to new data.\n",
    "\n",
    "Regularization techniques help address the bias-variance tradeoff by controlling the complexity of the model and reducing overfitting. Here's how regularization helps:\n",
    "\n",
    "1. Bias Reduction: Regularization techniques such as L1 and L2 regularization add a penalty term to the loss function, which discourages large weights in the model. By penalizing complex models, regularization helps reduce bias and prevents underfitting.\n",
    "\n",
    "2. Variance Reduction: Regularization techniques discourage overly complex models by penalizing large weights or by imposing constraints on the model parameters. This helps prevent the model from fitting the noise in the training data and promotes smoother decision boundaries, reducing variance and preventing overfitting.\n",
    "\n",
    "3. Improved Generalization: By controlling the tradeoff between bias and variance, regularization techniques help build models that generalize well to new, unseen data. Regularized models are less likely to memorize the training data and are better able to capture the underlying patterns, leading to improved performance on unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d58a491-50aa-43be-a0de-4b6b00e2b8b4",
   "metadata": {},
   "source": [
    "Q3. Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and their effects on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f3cb5-0438-476f-82e0-302ba706501a",
   "metadata": {},
   "source": [
    "L1 and L2 regularization are two common techniques used to prevent overfitting in machine learning models by adding a penalty term to the loss function. Here's a description of each:\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "\n",
    "In L1 regularization, the penalty term added to the loss function is proportional to the sum of the absolute values of the model weights.\n",
    "The penalty term can be expressed as λ * ||w||₁, where λ is the regularization parameter and ||w||₁ represents the L1 norm (sum of absolute values) of the model weights.\n",
    "\n",
    "The L1 regularization term encourages sparsity in the weight matrix, meaning it tends to force some of the weights to be exactly zero. This can lead to feature selection, where less important features are effectively ignored by the model.\n",
    "\n",
    "Due to its tendency to produce sparse solutions, L1 regularization can be useful when dealing with high-dimensional datasets with many irrelevant or redundant features.\n",
    "\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "In L2 regularization, the penalty term added to the loss function is proportional to the sum of the squared magnitudes of the model weights.\n",
    "\n",
    "The penalty term can be expressed as λ * ||w||₂², where λ is the regularization parameter and ||w||₂ represents the L2 norm (Euclidean norm or squared root of sum of squares) of the model weights.\n",
    "\n",
    "The L2 regularization term penalizes large weights in the model and encourages them to be small but non-zero. It discourages extreme parameter values and promotes smoother models by preventing individual weights from becoming too large.\n",
    "\n",
    "L2 regularization is particularly effective at preventing overfitting by reducing the model's sensitivity to small fluctuations or noise in the training data.\n",
    "\n",
    "Unlike L1 regularization, L2 regularization does not typically lead to sparse solutions; instead, it distributes the penalty across all the model weights, shrinking them towards zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f015b5a-15fa-409c-96e8-6b0532248ff7",
   "metadata": {},
   "source": [
    "Q4. Discuss the role of regularization in preventing overfitting and improving the generalization of deep\n",
    "learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d5d57-21a5-4e47-8545-6393c5b21a78",
   "metadata": {},
   "source": [
    "Regularization plays a crucial role in preventing overfitting and improving the generalization of deep learning models. Overfitting occurs when a model learns to memorize the training data too well, capturing noise and irrelevant patterns rather than generalizing to unseen data. Regularization techniques help address overfitting by adding constraints or penalties to the model during training, encouraging it to learn simpler and more generalizable representations. Here's how regularization achieves these goals in the context of deep learning:\n",
    "\n",
    "1. Control Model Complexity: Regularization techniques, such as L1 and L2 regularization, add penalty terms to the loss function that penalize large weights or complex model architectures. By penalizing complexity, regularization discourages the model from fitting the noise or specific examples in the training data too closely, leading to a simpler and more generalizable model.\n",
    "\n",
    "2. Prevent Overfitting: Regularization helps prevent overfitting by reducing the model's ability to capture noise or irrelevant patterns in the training data. Techniques like dropout randomly drop units (neurons) during training, effectively introducing noise and preventing the model from relying too heavily on any particular feature or combination of features.\n",
    "\n",
    "3. Encourage Robust Representations: Regularization encourages the learning of robust representations that generalize well to new, unseen data. By penalizing overly complex models or encouraging sparsity, regularization forces the model to focus on the most important features and learn representations that are more invariant to variations in the input data.\n",
    "\n",
    "4. Improve Generalization: By preventing overfitting and encouraging the learning of simpler and more robust representations, regularization ultimately improves the model's ability to generalize to new, unseen data. Regularized models are less likely to memorize the training data and more likely to capture the underlying patterns that generalize across different examples.\n",
    "\n",
    "5. Optimize Hyperparameters: Regularization techniques often involve hyperparameters, such as the regularization parameter (λ) in L1 and L2 regularization or the dropout rate in dropout regularization. Properly tuning these hyperparameters through techniques like cross-validation helps optimize the trade-off between bias and variance, leading to models with improved generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7ff52-1ee2-4973-bd1e-8aad1786c599",
   "metadata": {},
   "source": [
    "                            Part 2: Regularizatiop Tecpique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc79c68-0045-4901-9f59-40f40220b6de",
   "metadata": {},
   "source": [
    "Q5. Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on\n",
    "model training and inference.\n",
    "\n",
    "Dropout regularization is a technique used in deep learning to prevent overfitting by randomly dropping out (i.e., setting to zero) a fraction of the neurons or units in a neural network during training. The dropped-out units are not considered during a particular forward or backward pass of the network, effectively introducing noise and preventing the network from relying too heavily on any specific set of features. Here's how dropout regularization works and its impact on model training and inference:\n",
    "\n",
    "Training Phase:\n",
    "\n",
    "     During training, dropout is applied by randomly setting the outputs of a fraction (typically between 0.2 and 0.5) of the neurons to zero at each training iteration. This dropout probability determines the probability that any individual neuron is dropped out.\n",
    "\n",
    "    By randomly dropping out neurons, dropout effectively creates an ensemble of smaller subnetworks within the full network, each trained on a different subset of the input features.\n",
    "\n",
    "    The network's weights are updated based on the predictions and errors computed using the reduced network architecture, effectively preventing co-adaptation of neurons and promoting robustness.\n",
    "    \n",
    "    Inference Phase:\n",
    "\n",
    "    During inference or prediction, dropout is typically turned off, and all neurons are used. However, to account for the fact that more neurons were active during training, the weights of the neurons are scaled by the dropout probability. This scaling ensures that the expected output of the network remains the same across training and inference.\n",
    "\n",
    "    Alternatively, some implementations perform dropout during inference but scale the outputs by the dropout probability to maintain consistency with the training phase.\n",
    "    \n",
    "    \n",
    "    Impact of Dropout on Model Training and Inference:\n",
    "    \n",
    "1. Reduced Overfitting: Dropout helps prevent overfitting by regularizing the model and reducing its reliance on specific features or combinations of features. By randomly dropping out neurons during training, dropout encourages the network to learn more robust and generalizable representations.\n",
    "\n",
    "2. Improved Generalization: Dropout encourages the network to learn more diverse and invariant features by preventing it from memorizing the training data too closely. This leads to improved generalization performance on unseen data.\n",
    "\n",
    "3. Slower Convergence: Dropout can sometimes lead to slower convergence during training since the network is effectively training multiple smaller subnetworks. However, this slower convergence is often outweighed by the regularization benefits in terms of improved generalization performance.\n",
    "\n",
    "4. Ensemble Learning: Dropout can be viewed as a form of ensemble learning, where multiple models (the subnetworks created by dropout) are trained simultaneously and combined to make predictions. This ensemble effect helps reduce variance and improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d67b4-8edb-4c1f-93f6-893db7691c60",
   "metadata": {},
   "source": [
    "Q6. Describe the concept of Early ztopping as a form of regularization. How does it help prevent overfitting\n",
    "during the training process.\n",
    "\n",
    "Early stopping is a form of regularization used in machine learning, particularly in deep learning, to prevent overfitting during the training process. The idea behind early stopping is to monitor the model's performance on a separate validation dataset during training and stop the training process once the performance on the validation dataset starts to degrade. Here's how early stopping works and how it helps prevent overfitting:\n",
    "\n",
    "1. Training Process:\n",
    "\n",
    "During the training process, the model's performance metrics (e.g., loss, accuracy) are monitored on both the training dataset and a separate validation dataset.\n",
    "The model is trained iteratively using an optimization algorithm (e.g., stochastic gradient descent) to minimize the loss function on the training dataset.\n",
    "\n",
    "2. Validation Monitoring:\n",
    "\n",
    "At regular intervals (e.g., after each epoch), the model's performance is evaluated on the validation dataset.\n",
    "The performance metrics on the validation dataset are tracked over time, allowing us to monitor whether the model is improving or degrading in terms of its ability to generalize to new, unseen data.\n",
    "\n",
    "3. Early Stopping Criteria:\n",
    "\n",
    "Early stopping involves defining a stopping criterion based on the performance on the validation dataset. Common stopping criteria include:\n",
    "No improvement: Stop training if the performance on the validation dataset does not improve for a certain number of consecutive epochs.\n",
    "Performance degradation: Stop training if the performance on the validation dataset starts to degrade (e.g., if the validation loss increases) after an initial improvement.\n",
    "Threshold-based: Stop training if the performance on the validation dataset exceeds a certain threshold.\n",
    "\n",
    "4. Termination of Training:\n",
    "\n",
    "Once the stopping criterion is met, the training process is halted, and the model weights at the point of termination are typically saved.\n",
    "The model weights at the point of termination are often chosen as the final model, as they represent the point at which the model achieved the best performance on the validation dataset.\n",
    "\n",
    "How Early Stopping Helps Prevent Overfitting:\n",
    "\n",
    "1. Prevents Overfitting: Early stopping prevents overfitting by stopping the training process before the model has had a chance to memorize the training data too closely. By monitoring the model's performance on a separate validation dataset, early stopping ensures that the model generalizes well to new, unseen data.\n",
    "\n",
    "2. Generalization: Early stopping encourages the model to learn generalizable patterns in the data rather than fitting the noise or specific examples from the training dataset. By stopping the training process at the point of best validation performance, early stopping helps select a model that performs well on unseen data.\n",
    "\n",
    "3. Regularization Effect: Early stopping can be viewed as a form of regularization because it prevents the model from becoming overly complex and overfitting to the training data. By terminating the training process early, early stopping effectively limits the model's capacity and encourages it to learn simpler and more generalizable representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5b4aa-73f5-4f65-bab9-a41b2d434ea9",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch\n",
    "Normalization help in preventing overfitting.\n",
    "\n",
    "Batch Normalization is a technique used in deep learning to address a common issue known as internal covariate shift, where the distribution of activations in a network changes as the parameters of the preceding layers are updated during training. Batch Normalization mitigates this problem by normalizing the inputs to each layer in a neural network across mini-batches. In addition to its role in improving training stability and accelerating convergence, Batch Normalization also serves as a form of regularization, helping to prevent overfitting. Here's how Batch Normalization works and its role in regularization:\n",
    "\n",
    "1. Normalization:\n",
    "\n",
    "For each mini-batch during training, Batch Normalization normalizes the activations of each layer by subtracting the mean and dividing by the standard deviation computed across the mini-batch.\n",
    "\n",
    "The normalized activations are then scaled and shifted using learnable parameters (gamma and beta) to allow the network to learn the optimal scale and shift for each feature.\n",
    "\n",
    "2. Stabilization of Training:\n",
    "\n",
    "By normalizing the activations, Batch Normalization helps stabilize the training process by reducing internal covariate shift. This allows the network to more effectively learn the parameters of each layer and accelerates convergence, leading to faster training.\n",
    "\n",
    "3. Regularization Effect:\n",
    "\n",
    "Batch Normalization introduces noise into the network by normalizing activations across mini-batches. This noise acts as a form of regularization, similar to techniques like dropout, by adding randomness to the training process.\n",
    "\n",
    "The noise introduced by Batch Normalization helps prevent the network from relying too heavily on specific features or activations, reducing the risk of overfitting.\n",
    "\n",
    "Additionally, the normalization of activations helps ensure that the network learns more robust and generalizable representations, further contributing to regularization.\n",
    "\n",
    "4. Reduced Sensitivity to Initialization:\n",
    "\n",
    "Batch Normalization reduces the sensitivity of the network to the choice of initialization parameters. By normalizing activations, it helps ensure that the network converges more consistently across different initialization schemes, making training more stable.\n",
    "\n",
    "5. Smoothing the Loss Landscape:\n",
    "\n",
    "Batch Normalization smooths the loss landscape, making it less likely for the optimization process to get stuck in sharp, narrow minima. This effect helps prevent the network from overfitting to the training data and encourages it to find broader and more generalizable solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def707a-8d5c-455b-aa35-7753f56bd090",
   "metadata": {},
   "source": [
    "                                Part 3: Applyipg Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3dd94-de21-488c-bce8-24807a8099f7",
   "metadata": {},
   "source": [
    "Q8. Implement Dropout regularization in a deep learning model using a framework of your choice. Evaluate\n",
    "its impact on model performance and compare it with a model without Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87db60c-d469-4afe-ac7f-b7609f02785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: pin: command not found\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.60.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.1)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.27.0 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.60.1 keras-2.15.0 libclang-16.0.6 markdown-3.5.2 ml-dtypes-0.2.0 opt-einsum-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pin install numpy\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60555b8-1932-4078-b083-bfae17ae3a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 07:43:12.403319: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-07 07:43:12.467763: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-07 07:43:12.467838: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-07 07:43:12.469234: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-07 07:43:12.478007: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-07 07:43:12.478662: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-07 07:43:13.664950: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20bc4a14-df3c-4531-8645-11da7f4ccec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(use_dropout=False):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(784,)),\n",
    "        Dropout(0.2) if use_dropout else None,  # Dropout layer\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.2) if use_dropout else None,  # Dropout layer\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32ecf96f-c3f0-4d65-ab33-ce896735e4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Flatten the images\n",
    "x_train = x_train.reshape((-1, 784))\n",
    "x_test = x_test.reshape((-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc77f8c5-f313-4237-ab4e-1d92a78522d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Received: layer=None of type <class 'NoneType'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_no_dropout \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m history_no_dropout \u001b[38;5;241m=\u001b[39m model_no_dropout\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(x_test, y_test))\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(use_dropout)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model\u001b[39m(use_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Dropout layer\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Dropout layer\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m                   loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m                   metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/sequential.py:178\u001b[0m, in \u001b[0;36mSequential.add\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    176\u001b[0m         layer \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39mModuleWrapper(layer)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe added layer must be an instance of class Layer. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: layer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    183\u001b[0m tf_utils\u001b[38;5;241m.\u001b[39massert_no_legacy_layers([layer])\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_layer_name_unique(layer):\n",
      "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Received: layer=None of type <class 'NoneType'>."
     ]
    }
   ],
   "source": [
    "model_no_dropout = create_model(use_dropout=False)\n",
    "history_no_dropout = model_no_dropout.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d41a07f-4b97-4442-8849-41eab7c9d76a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Received: layer=None of type <class 'NoneType'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_no_dropout \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m history_no_dropout \u001b[38;5;241m=\u001b[39m model_no_dropout\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(x_test, y_test))\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(use_dropout)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model\u001b[39m(use_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Dropout layer\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Dropout layer\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m                   loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m                   metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/engine/sequential.py:178\u001b[0m, in \u001b[0;36mSequential.add\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    176\u001b[0m         layer \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39mModuleWrapper(layer)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe added layer must be an instance of class Layer. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: layer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    183\u001b[0m tf_utils\u001b[38;5;241m.\u001b[39massert_no_legacy_layers([layer])\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_layer_name_unique(layer):\n",
      "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Received: layer=None of type <class 'NoneType'>."
     ]
    }
   ],
   "source": [
    "model_no_dropout = create_model(use_dropout=False)\n",
    "history_no_dropout = model_no_dropout.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3fc09-47a1-4ac3-a033-4be0745e3d95",
   "metadata": {},
   "source": [
    "Q9. Discuss the considerations and tradeoffs when choosing the appropriate regularization technique for a\n",
    "given deep learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046a90d-67fd-4431-818d-6edb70be4056",
   "metadata": {},
   "source": [
    "Discuss the considerations and tradeoffs when choosing the appropriate regularization technique for a\n",
    "given deep learning task.\n",
    "\n",
    "1. Dataset Size:\n",
    "\n",
    "For small datasets, simpler regularization techniques like L1 and L2 regularization may be sufficient to prevent overfitting without introducing too much computational overhead.\n",
    "For larger datasets, more sophisticated techniques like dropout or data augmentation may be necessary to prevent overfitting and improve model generalization.\n",
    "\n",
    "2. Model Complexity:\n",
    "\n",
    "More complex models are prone to overfitting, so stronger regularization techniques may be required to prevent overfitting.\n",
    "Regularization techniques like dropout and batch normalization are particularly effective for deep neural networks with many layers, as they help stabilize training and prevent overfitting.\n",
    "\n",
    "3. Computational Resources:\n",
    "\n",
    "Some regularization techniques, such as dropout and data augmentation, can increase the computational cost of training by introducing additional computations or requiring more training epochs.\n",
    "Consider the available computational resources and training time constraints when choosing regularization techniques.\n",
    "\n",
    "4. Interpretability:\n",
    "\n",
    "Regularization techniques like L1 regularization (Lasso) promote sparsity in the model weights, making them more interpretable by encouraging feature selection.\n",
    "If model interpretability is important, L1 regularization may be preferred over other techniques.\n",
    "\n",
    "5. Training Speed:\n",
    "\n",
    "Some regularization techniques, such as dropout, may slow down the training process by introducing randomness or requiring more iterations to converge.\n",
    "Consider the tradeoff between training speed and regularization effectiveness when choosing techniques.\n",
    "\n",
    "6. Domain-Specific Considerations:\n",
    "\n",
    "Consider the specific characteristics of the problem domain and the nature of the data when choosing regularization techniques.\n",
    "For example, if the dataset contains noisy or unstructured data, techniques like dropout and batch normalization may be particularly effective.\n",
    "\n",
    "7. Ensemble Techniques:\n",
    "\n",
    "Ensemble techniques, such as bagging and boosting, can also be used as regularization methods by training multiple models and combining their predictions.\n",
    "\n",
    "Ensemble techniques can improve model generalization by reducing variance and capturing diverse patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8821a-a650-4ae9-8e3c-4846c195b44b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93b3b6-c3f5-4738-bdbd-ad12dd17b48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492e3d8-140d-4093-bf27-8f2fc59a7c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d4915-caf7-4dac-8889-af1def389800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
